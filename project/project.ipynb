{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsal\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from twarc import Twarc\n",
    "from geotext import GeoText\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pycountry\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%pylab inline\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Data Collection</h1>\n",
    "\n",
    "<h2>1a) Collecting Trump tweets</h2>\n",
    "\n",
    "We utilised the several datasets available at https://github.com/bpb27/trump_tweet_data_archive which happen to be automatically updated every hour. All of the datasets are saved in json format and the data contained in them is the result of calls to the Twitter API. There are two varients, <b>master</b> which contains all of the original information from the API call and <b>condensed</b> which contains a lot less but essential information. After looking through some of the master and condensed datasets, we decided that the condensed datasets were sufficient as they contained the specfic fields we required and the entries in master were unnecessarily large for our purpose (too much information that we would not use). <br>\n",
    "\n",
    "We're using all of the condensed datasets from 2009 to 2017 because the tweet data isn't that time consuming to process and having as much data as possible will be beneficial when clustering the messages in the tweets. Since the 2017 datatset is continiously being updated, we are only using it up to a certain date. All of the data in total is around 10 MB so it is really simple to handle and there is no need to rely on additional computing resources<br>\n",
    "\n",
    "Below, we iterate over each json file and turn them into a pandas DataFrame and store them in the list <b>data</b>. We do this so that later on we can simply combine them into one single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create empty list to store all of our data\n",
    "data=[]\n",
    "\n",
    "#Add data from 2009 to 2017, 2017 data obtained 28/11/2017\n",
    "data.append(pd.DataFrame(json.load(open(\"data/condensed_2009.json\"))))\n",
    "for i in range(10,18):\n",
    "    fileName=\"data/condensed_20\"+str(i)+\".json\"\n",
    "    data.append(pd.DataFrame(json.load(open(fileName))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine all of our seperate datasets into one pandas dataframe. We also set the id of the dataframe to <b>id_str</b> since we already know that each of the ids are unique. we also have to convert the <b>created_at</b> attribute into a Date object so that the dataframe is able to properly sort tweets by date (if it was still a string then it would sort incorrectly as it would put tweets starting on a Friday first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine all of the individual datasets together\n",
    "df = pd.concat(data)\n",
    "\n",
    "#Set the index\n",
    "df = df.set_index(\"id_str\")\n",
    "\n",
    "#Convert to Date\n",
    "df[\"created_at\"]=pd.to_datetime(df[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>2009-05-04 18:54:25</td>\n",
       "      <td>202</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>253</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>2009-05-05 01:00:10</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>2009-05-08 13:38:08</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>2009-05-08 20:40:15</td>\n",
       "      <td>27</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>2009-05-12 14:07:28</td>\n",
       "      <td>1950</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1421</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at  favorite_count in_reply_to_user_id_str  \\\n",
       "id_str                                                                   \n",
       "1698308935 2009-05-04 18:54:25             202                    None   \n",
       "1701461182 2009-05-05 01:00:10               3                    None   \n",
       "1737479987 2009-05-08 13:38:08               2                    None   \n",
       "1741160716 2009-05-08 20:40:15              27                    None   \n",
       "1773561338 2009-05-12 14:07:28            1950                    None   \n",
       "\n",
       "            is_retweet  retweet_count              source  \\\n",
       "id_str                                                      \n",
       "1698308935       False            253  Twitter Web Client   \n",
       "1701461182       False              2  Twitter Web Client   \n",
       "1737479987       False              3  Twitter Web Client   \n",
       "1741160716       False              8  Twitter Web Client   \n",
       "1773561338       False           1421  Twitter Web Client   \n",
       "\n",
       "                                                         text  \n",
       "id_str                                                         \n",
       "1698308935  Be sure to tune in and watch Donald Trump on L...  \n",
       "1701461182  Donald Trump will be appearing on The View tom...  \n",
       "1737479987  Donald Trump reads Top Ten Financial Tips on L...  \n",
       "1741160716  New Blog Post: Celebrity Apprentice Finale and...  \n",
       "1773561338  \"My persona will never be that of a wallflower...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"created_at\",inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1b) Reply tweets data</h2>\n",
    "\n",
    "In order to do a sentiment analysis, we need to create a dataframe containing replies to Trump's tweets. To do this, we use the library Twarc which simplifies the API call process. Note that due to a limitation in the Twitter API, getting replies to a tweet is not a feature by default so you have to use a workaround. One thing that's commonly done is to use Twitter's search function, it would involve searching for all messages directed at Trump and then seeing which ones have a reply_id (the id of the tweet they are replying to) that is equal to a given Trump tweet id. The problem with this is that it is only able to look for replies to tweets that are max a week old. The second issue is that the amount of times that you can use the search function in the Twitter API is limited to a 100 queries an hour which is not ideal since it would take too long to get enough tweets to train our classifiers.\n",
    "\n",
    "Instead, we decided to do the following. We iterate over all of Trump's tweets, get the id of these tweets and then do a http request to the twitter page containing that tweet. Then we parse the page using BeautifulSoup and search for the attribute \"data-tweet-id\" in the HTML text. This attribute corresponds to an id of any tweet on that page which means that we can easily obtain the id of replies. Then, instead of using the search function in the Twitter API, we use a different method that can lookup tweet information given an id. This method is not limited to tweets that are more than 7 days old and has a higher rate limit. \n",
    "\n",
    "We are using the package Twarc and to use it we need authenticate it with keys and access tokens that can be obtained from the Twitter Dev website.\n",
    "\n",
    "The keys won't be available in GitHub for security reasons, instead we read them from a local file. The format is one line where\n",
    "each key is seperated by a \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname=\"data/authKeys.txt\"\n",
    "f = open(fname, \"r\")\n",
    "\n",
    "keys=f.read()[2:].split(\":\")\n",
    "\n",
    "for i in keys:\n",
    "    i=i.replace(\"\\x00\",\"\")\n",
    "\n",
    "t = Twarc(keys[0],keys[1],keys[2],keys[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are building a dataframe to store the reply texts as well as other information we might find useful. Since the webscraping process takes time, we felt that it was better to prioritize the latest 2000 tweets as they are more relevant than tweets that were made before Trump's presidency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#A list of lists for creating the 2D dataframe\n",
    "L=[]\n",
    "\n",
    "#Column names for our dataframe containing replies\n",
    "indexList=[\"reply_id\",\"trump_id\",\"created_at\",\"coordinates\",\"favorite_count\",\"retweet_count\",\"geo\",\n",
    "                              \"place\",\"full_text\",\"location\",\"time_zone\",\"utc_offset\"]\n",
    "\n",
    "#Add a list for each feature\n",
    "for i in range(len(indexList)):\n",
    "    L.append([])\n",
    "\n",
    "#Create empty dataframe\n",
    "replyDf=pd.DataFrame(columns=indexList)\n",
    "\n",
    "#Sort trump dataframe to get recent first\n",
    "df.sort_values(\"created_at\",inplace=True,ascending=False)\n",
    "\n",
    "#Adds the information from the reply tweet into our list of lists L\n",
    "def CreateReplyEntry(trumpId,replyId,replyTweet):\n",
    "    L[0].append(replyId)\n",
    "    L[1].append(trumpId)\n",
    "    for i in range(len(indexList)):\n",
    "        if(i>1):\n",
    "            if(i>8): #This if exists because of how the Json string replyTweet is formed\n",
    "                L[i].append(replyTweet[\"user\"][indexList[i]])\n",
    "            else:\n",
    "                L[i].append(replyTweet[indexList[i]])\n",
    "\n",
    "'''\n",
    "For every tweet in given range (start,end), this finds several replies\n",
    "to that tweet (not guaranteed to find all replies)\n",
    "start and end are indexes in trump tweets dataframe\n",
    "'''\n",
    "def ScrapeReplies(start,end):\n",
    "    #Iterate over every tweet\n",
    "    for i in range(start,end):\n",
    "        if(i%100==0):\n",
    "            print(\"Tweet: \"+str(i))\n",
    "\n",
    "        #Get trump tweet and id\n",
    "        tweet=df.iloc[i]\n",
    "        tweetId=df.index[i]\n",
    "\n",
    "        url=\"https://twitter.com/realDonaldTrump/status/\"+tweetId\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        #Finds all div tags and if it has the corresponding id, add that id to a list\n",
    "        soup.find_all(\"div\")\n",
    "        divs = soup.find_all(\"div\")\n",
    "        ids=[]\n",
    "        for d in divs:\n",
    "            if d.has_attr('data-tweet-id'):\n",
    "                ids.append(d[\"data-tweet-id\"])  \n",
    "\n",
    "        '''\n",
    "        ids is a list of all tweets on the url, we want to \n",
    "        ignore Trump's tweet id and only do a lookup for the replies        \n",
    "        '''\n",
    "        for i in ids:\n",
    "            if(i!=tweetId): #ignore trump tweet id\n",
    "                reply=t.tweet(i)\n",
    "                CreateReplyEntry(tweetId,i,reply)   \n",
    "\n",
    "    #Fills the dataframe with information stored in L\n",
    "    for i in range(len(indexList)):\n",
    "        replyDf[indexList[i]]=L[i]\n",
    "\n",
    "    #Write dataframe to local file\n",
    "    replyDf.to_pickle(\"data/replies/replies_\"+str(start)+\"_\"+str(end)+\".pkl\")\n",
    "    print(\"replies for tweets \"+str(start)+\" to \"+str(end)+\" saved\")\n",
    "\n",
    "#Use line below to scrape replies (can take 1 to 2 hours to complete)\n",
    "#ScrapeReplies(0,2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Analyzing the data</h1><br>\n",
    "\n",
    "<h2>2a) Attributes in the condensed Trump data</h2>\n",
    "\n",
    "What does the data look like ? In each json file, there is an array where each element represents\n",
    "a seperate tweet. In each array element, the following information can be found:<br>\n",
    "\n",
    "<ul>\n",
    "<li><b>created_at:</b> (string) the date the tweet was posted<br></li>\n",
    "<li><b>favourite_count:</b> (integer) represents the number of individual users that liked this post<br></li>\n",
    "<li><b>id_str:</b> (string) a unique string id for this tweet provided by the API<br></li>\n",
    "<li><b>in_reply_to_user_id_str:</b> (string) the string id of a tweet that this tweet is a reply to<br></li>\n",
    "<li><b>is_retweet:</b> (boolean) True if this tweet is a retweet of another tweet <br></li>\n",
    "<li><b>retweet_count:</b> (integer) an integer representing the amount of times this tweet was retweeted<br></li>\n",
    "<li><b>source:</b> (string/categorical) the type of device this tweet was posted from (for example a desktop or phone)<br></li>\n",
    "<li><b>text:</b> (string) the actual text from this tweet<br></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all of the possible values of <b>source</b>, we simply use the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Twitter for iPhone', 'Twitter for iPad', 'Media Studio',\n",
       "       'Twitter Web Client', 'Twitter Ads', 'Twitter for Android',\n",
       "       'Periscope', 'TweetDeck', 'Instagram', 'Mobile Web (M5)',\n",
       "       'Twitter Mirror for iPad', 'Twitter QandA', 'Facebook',\n",
       "       'Twitter for BlackBerry', 'Neatly For BlackBerry 10',\n",
       "       'Twitter for Websites', 'Twitlonger', 'Vine - Make a Scene',\n",
       "       'TwitLonger Beta'], dtype=object)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"source\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>favourite_count</b> and <b>retweet_count</b> are both positive integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of what 1 entry may look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                                     2017-11-28 02:50:30\n",
       "favorite_count                                               25703\n",
       "in_reply_to_user_id_str                                       None\n",
       "is_retweet                                                   False\n",
       "retweet_count                                                 6307\n",
       "source                                          Twitter for iPhone\n",
       "text                       Thank you Rand! https://t.co/NvPeleVmub\n",
       "Name: 935340092583006208, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to be noted about twitter for those who are unfamiliar, a retweet is essentially where you take someone else's tweet and post that to your twitter feed where as a reply is a tweet that is directly aimed at an existing tweet. \n",
    "\n",
    "What we can see from the dataset above is that not all of the information we need is there. For example to do a proper sentiment analysis of people reacting to Trump, we need to look at the replies of these people. Also, we need to see what country those replies originated from. This can be done using the twitter API since the API calls contain more extensive information such as geolocation of the tweet's origin. \n",
    "\n",
    "If we take a look at the tweets themselves, we see that there are certain symbols and text that we can remove as they don't contribute to our clustering of topics. For example, a lot of tweets contain external http links. Also, twitter happens to encode the & symbol as the string &amp so these need to be converted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2b) Computing statistical information</h2>\n",
    "\n",
    "Using the describe function below, we can see that there are 30192 tweets in our total dataset. Only two of the features can have these values computed because the rest are strings or booleans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32547.000000</td>\n",
       "      <td>32547.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8458.899315</td>\n",
       "      <td>2582.873383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25235.173044</td>\n",
       "      <td>7438.815940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>633253.000000</td>\n",
       "      <td>369530.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       favorite_count  retweet_count\n",
       "count    32547.000000   32547.000000\n",
       "mean      8458.899315    2582.873383\n",
       "std      25235.173044    7438.815940\n",
       "min          0.000000       0.000000\n",
       "25%         20.000000      16.000000\n",
       "50%         68.000000     107.000000\n",
       "75%       1922.000000    1157.000000\n",
       "max     633253.000000  369530.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naYiISGRKGiIiEpmShoiIRPb/ASWrJFV9E3AcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a4f2909278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHwbI/jSMorkfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a4f24bf1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(df[\"favorite_count\"]))\n",
    "plt.show()\n",
    "plt.plot(list(df[\"retweet_count\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2c) Checking for missing values</h2>\n",
    "\n",
    "We will now check to see if there are potentially any values missing at all. Note that we don't consider a value of None in <b>in_reply_to_user_id_str</b> to be a missing value as it's acceptable for a tweet to not reply to another. We make a copy of our original dataframe, drop the in_reply_to_user_id_str (because pandas replcae the value None with a null value in this case) and then do a query to return all entries with some form of missing value. As we can see, we get no entries in this case so all of the values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, favorite_count, is_retweet, retweet_count, source, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNull=df.copy()\n",
    "dfNull=dfNull.drop(\"in_reply_to_user_id_str\",axis=1)\n",
    "dfNull[dfNull.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2d) Correlations between features</h2>\n",
    "\n",
    "Next we want to analyze the correlations between the different fields. It doesn't make sense to look at correlations between string fields and the rest as the strings can take on a large range of values that aren't repeated often. However we can convert the categorical strings into integers as they occur frequently. We create a copy of the dataframe and then create a dictionary to have a mapping from a <b>source</b> value to an integer. Then we alter all of the rows in this datafram to have these integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfCorr=df.copy()\n",
    "\n",
    "keys=dfCorr[\"source\"].unique()\n",
    "values=list(range(len(keys)))\n",
    "dictionary = dict(zip(keys, values))\n",
    "\n",
    "for i in range(len(dfCorr)):    \n",
    "    s=dfCorr.iloc[i][\"source\"]\n",
    "    dfCorr.iloc[i, dfCorr.columns.get_loc('source')] = int(dictionary.get(s))\n",
    "    \n",
    "dfCorr[\"source\"] = dfCorr[\"source\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                 datetime64[ns]\n",
       "favorite_count                      int64\n",
       "in_reply_to_user_id_str            object\n",
       "is_retweet                           bool\n",
       "retweet_count                       int64\n",
       "source                              int64\n",
       "text                               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCorr.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a correlation matrix between all of the integer values as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>favorite_count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040312</td>\n",
       "      <td>0.903915</td>\n",
       "      <td>-0.281563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_retweet</th>\n",
       "      <td>-0.040312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100370</td>\n",
       "      <td>-0.170955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweet_count</th>\n",
       "      <td>0.903915</td>\n",
       "      <td>0.100370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.275444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>-0.281563</td>\n",
       "      <td>-0.170955</td>\n",
       "      <td>-0.275444</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                favorite_count  is_retweet  retweet_count    source\n",
       "favorite_count        1.000000   -0.040312       0.903915 -0.281563\n",
       "is_retweet           -0.040312    1.000000       0.100370 -0.170955\n",
       "retweet_count         0.903915    0.100370       1.000000 -0.275444\n",
       "source               -0.281563   -0.170955      -0.275444  1.000000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrMatr=dfCorr.corr(method='pearson', min_periods=1)\n",
    "corrMatr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngUCEDwQifCDQfwC3lMrR0m071QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a4f39796a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(corrMatr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the matrix, yellow is for 100% correlation (when a variable correlates to itself), the more darker colours represent less correlation and the green one show that there is a relatively high correlation (>90%). What we see is that for Trump's tweets, the <b>favorite_count</b> and <b>retweet_count</b> are highly correlated. The correlations between the other variables are quite low (from -0.021752 to 0.134390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replies=pd.read_pickle(\"data/replies/replies_0_2000.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analyzing the replies data</h2>\n",
    "\n",
    "The main challenge with the replies dataset we created is that not all of the features we need may be present as well as the fact that user's seem to have full control over what their locations can be. This is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0973% are not null\n",
      "0.0973% are not null\n",
      "2.2024% are not null\n",
      "70.2924% are not null\n",
      "49.2094% are not null\n",
      "100.0% are not null\n"
     ]
    }
   ],
   "source": [
    "def CountNotNull(r):\n",
    "    notNone=0\n",
    "    for i in r.values:\n",
    "        if(not i==None)&(not i==\"\"):\n",
    "            notNone+=1\n",
    "    print(str(round(notNone/len(r.values)*100,4))+\"% are not null\")\n",
    "    \n",
    "CountNotNull(replies[\"coordinates\"])\n",
    "CountNotNull(replies[\"geo\"])\n",
    "CountNotNull(replies[\"place\"])\n",
    "CountNotNull(replies[\"location\"])\n",
    "CountNotNull(replies[\"time_zone\"])\n",
    "CountNotNull(replies[\"utc_offset\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using coordinates, geo or place field is not sufficient as a lot of the values are missing. time_zone has 51% of values missing and even if the existing values where used, they are not precise enough as several countries can share the same time_zone. utc_offset share the same problem as time_zone and is only useful if it were used to verify a location (for example if the location is Las Vegas but the UTC offset is 0 (same time zone as London) then we know the location is incorrect. So we decided to rely on using the location field\n",
    "\n",
    "The problem with the location field is that users are able to enter any string as their location. This causes the following issues:\n",
    "\n",
    "<ul>\n",
    "<li><b>Multiple strings referring to the same location:</b> The strings \"USA\", \"America\", \"United States\" for example all refer\n",
    "to the same location, the United States. For our visualisations to work, a 2 or 3 country code or the name of the country\n",
    "needs to be supplied. So one task will be to map several strings to the same location.</li>\n",
    "<li><b>Strings that aren't useful locations:</b>Since user's have a choice in what their location is, some may decide to set their location to something that doesn't provide any meaningful information (such as \"here\"or \"in my house\"). Also, some users may put their location as continents such as Europe or North America and such locations are not specific enough since we want to analyze sentiments by country</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6303"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replies[\"location\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   9469\n",
       "United States      1265\n",
       "Washington, DC      455\n",
       "USA                 411\n",
       "Los Angeles, CA     404\n",
       "Fort Myers, FL      370\n",
       "Oakland, CA         322\n",
       "Florida, USA        311\n",
       "California, USA     305\n",
       "Washington, USA     237\n",
       "New York, USA       223\n",
       "Texas, USA          210\n",
       "New York, NY        180\n",
       "LA & NYC            151\n",
       "Global              148\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[\"location\"].value_counts()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that out of the 31874 reply tweets to handle, there are 6303 unique locations. This number is large enough to say that it would take too long to manually change each location to something valid. Also in the cell below we see that 4160 locations are only used once, 926 are used twice and so on. For the cases where a lot of tweets have the same location, it is easier to change as there are less cases to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4160\n",
       "2     926\n",
       "3     410\n",
       "4     238\n",
       "5     109\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[\"location\"].value_counts().value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3) Sentiment Analysis by Location</h1><br>\n",
    "\n",
    "There are several steps that need to be performed before we can compute the sentiment's of replies by country. First of all we need to ensure that all replies have the correct location. If the location of a tweet is a city, this needs to be converted to a country (Las Vegas becomes United States). Also, several locations referring to the same place should be mapped to a single value (USA and United States are the same place) so that aggregation can work properly. There are also a lot of locations that are part of a sentence (for example, \"Somewhere in Kansas\") so the locations need to be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3a) Fixing locations for replies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the replies\n",
    "replies=pd.read_pickle(\"data/replies/replies_0_2000.pkl\")\n",
    "\n",
    "locations=replies[\"location\"]\n",
    "locationsNotFound=locations.value_counts()\n",
    "\n",
    "#Dictionary that maps location to country code, needs to be filled up\n",
    "locationsDict={}\n",
    "\n",
    "def UseUsStates():\n",
    "    usStates=[\"AL\",\"AK\",\"AS\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"DC\",\"FM\",\"FL\",\"GA\",\"GU\",\"HI\",\"ID\",\n",
    "    \"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MH\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\n",
    "    \"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"MP\",\"OH\",\"OK\",\"OR\",\"PW\",\"PA\",\"PR\",\"RI\",\"SC\",\n",
    "    \"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VI\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"]\n",
    "\n",
    "    for x in locationsNotFound.index:\n",
    "        for s in x.split(\",\"):\n",
    "            if s.strip() in usStates:\n",
    "                Drop(x,\"US\")\n",
    "\n",
    "def ContainsUSA():\n",
    "    for x in locationsNotFound.index:\n",
    "        for s in x.split(\",\"):\n",
    "            if s.strip()==\"USA\":\n",
    "                Drop(x,\"US\")\n",
    "                \n",
    "def Drop(a,b):\n",
    "    locationsDict[a]=b\n",
    "    try:\n",
    "        locationsNotFound.drop(a,inplace=True)\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def SetUS(maxI):\n",
    "    i=1\n",
    "    for x in locationsNotFound.index:\n",
    "        if(i>maxI):\n",
    "            break\n",
    "        locationsDict[x]=\"US\"\n",
    "        locationsNotFound.drop(x,inplace=True)        \n",
    "        i+=1 \n",
    "        \n",
    "def RemoveInvalid():\n",
    "    for x in locationsNotFound.index:\n",
    "        if(\"@\" in x):\n",
    "            locationsDict[x]=\"unknown\"\n",
    "            locationsNotFound.drop(x,inplace=True)\n",
    "        if(x==\"\"):\n",
    "            locationsDict[x]=\"unknown\"\n",
    "            locationsNotFound.drop(x,inplace=True)\n",
    "            \n",
    "def UseCountryMentions():\n",
    "    for x in locationsNotFound.index:\n",
    "        countryMentions=GeoText(x).country_mentions\n",
    "\n",
    "        firstKey=\"\"\n",
    "        for c in countryMentions:\n",
    "            firstKey=c\n",
    "            break\n",
    "\n",
    "        if(len(countryMentions)<2)&(len(countryMentions)>0):\n",
    "            locationsDict[x]=firstKey\n",
    "            locationsNotFound.drop(x,inplace=True)\n",
    "            \n",
    "def DropUnknown():\n",
    "    unknown = [\"A Rabbit Hole\",\"Earth\",\"Planet Earth\",\"Global\",\"Everywhere\",\"Here\",\"Lost City of Atlantis\",\n",
    "           \"George Soros' basement\",\"Here.Now\",\"Planet Earth \",\"Non-Ya, Beezwax\",\"COOL CITY\",\"North Pole\",\"The Resistance\",\"Internationalist, No Pasaran\",\"Parts Unknown\",\n",
    "               \"Planet Earth\",\"Over there\",\"home\",\"GeeksResist Headquarters\",\"This beautiful planet\",\"Worldwide\",  \"Retweeted by Matt Schlapp CPAC\",   \n",
    "\"Somewhere beyond the sea...\",\n",
    "\"Reality\",\"Under your bed\",  \n",
    "\"So far, you just can't see.\",\n",
    "\"Always #PledgeTruth.  TrumpLuvers don't bother to follow me!  Haters will be met with a kindly block\",\n",
    "\"Retired Aviation Pilot\", \n",
    "\"All Tweets Are My Own\" \n",
    "              ]\n",
    "\n",
    "    for u in unknown:\n",
    "        locationsDict[u]=\"unknown\"\n",
    "        try:\n",
    "            t=locationsNotFound[u]\n",
    "            locationsNotFound.drop(u,inplace=True)\n",
    "        except:\n",
    "            b=2\n",
    "            \n",
    "def DropHardcoded():\n",
    "    Drop(\"Toronto, Ontario\",\"CA\")\n",
    "    Drop(\"UK\",\"GB\")\n",
    "    Drop(\"Deutschland\",\"DE\")\n",
    "    Drop(\"Ontario, Canada\",\"CA\")   \n",
    "    Drop(\"NSW\",\"AU\")\n",
    "    Drop('Trinidad and Tobago',\"TT\")\n",
    "    Drop(\"Newcastle upon Tyne, United Ki\",\"GB\")\n",
    "    usLoc=['Florida ',\"Florida, USA\",'FLORIDA','Florida, USA Earth','Florida','Palm Beach, Florida',\n",
    "       'palm coast Florida',\n",
    "        \"Virginia, USA\",\n",
    "        \"Virginia\",\n",
    "        \"Richmond, VA\",\n",
    "        \"Bethlehem, PA USA\",\n",
    "        \"Richmond, Virginia\",\n",
    "        \"Bethlehem, PA\",\n",
    "        \"Commonwealth of Virginia\",\n",
    "        \"Worcester, MA\",\n",
    "        \"Richmond, IN\",\n",
    "        \"Bethlehem, Pennsylvania\",\n",
    "        \"Henrico, Virginia, USA\",\n",
    "        \"Virginia - District 10\",\n",
    "        \"Hampton Roads, Virginia\",\n",
    "        \"Dundee,FL.\",\n",
    "        \"Shenandoah Valley, Virginia\",\n",
    "        \"Richmond, TX\",\n",
    "        \"Richmond, CA\",\n",
    "        \"Richmond, KY\",\n",
    "        \"Dundee, Ohio\",\n",
    "        \"Richmond, Virginia \",\n",
    "        \"Lansdowne, Pa US\",\n",
    "        \"McLean/Virginia\",\n",
    "        \"Warrenton, VA\",\n",
    "        \"Loudoun County, Virginia\",\n",
    "        \"Colorado, USA\",\n",
    "        \"Colorado\",\n",
    "        \"Colorado \",\n",
    "        \"Evergreen, Colorado\",\n",
    "        \"Southeast Plains of Colorado\",\n",
    "        \"eastern Colorado\",\n",
    "        \"Georgia, USA\",\n",
    "        \"Georgia\",\n",
    "        \"McDonough, Georgia\",\n",
    "        \"Georgia, USA 🇺🇸\",\n",
    "        \"  The World.    (Georgia, USA)\",\n",
    "        \"Palmetto, Georgia, USA\",\n",
    "        \"Houstonian in Georgia \"\n",
    "              ]\n",
    "\n",
    "    for u in usLoc:\n",
    "        Drop(u,\"US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UseUsStates()\n",
    "ContainsUSA()\n",
    "DropHardcoded()\n",
    "RemoveInvalid()\n",
    "DropUnknown()\n",
    "UseCountryMentions()\n",
    "SetUS(35)\n",
    "\n",
    "locations=replies[\"location\"]\n",
    "l=[]\n",
    "\n",
    "a=0\n",
    "for i in range(len(locations)):\n",
    "    loc=locations.iloc[i]\n",
    "    l.append(locationsDict.get(loc))\n",
    "locs=pd.Series(l)\n",
    "#locs.drop(\"unknown\",inplace=True)\n",
    "locs.value_counts().head()\n",
    "replies[\"location\"]=locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are able to see the distribution of the reply tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                        14718\n",
       "unknown                              10441\n",
       "United Kingdom                         453\n",
       "Canada                                 225\n",
       "Bangladesh                             213\n",
       "Australia                               78\n",
       "India                                   75\n",
       "Germany                                 73\n",
       "Botswana                                38\n",
       "China                                   32\n",
       "Nigeria                                 28\n",
       "Finland                                 27\n",
       "Yemen                                   24\n",
       "Brazil                                  23\n",
       "Uruguay                                 23\n",
       "Italy                                   22\n",
       "Israel                                  21\n",
       "France                                  19\n",
       "New Zealand                             18\n",
       "South Africa                            18\n",
       "Egypt                                   17\n",
       "Norway                                  16\n",
       "Pakistan                                16\n",
       "Sweden                                  16\n",
       "Japan                                   13\n",
       "Ireland                                 13\n",
       "Venezuela, Bolivarian Republic of       10\n",
       "Korea, Republic of                       9\n",
       "Puerto Rico                              9\n",
       "Morocco                                  9\n",
       "                                     ...  \n",
       "Guatemala                                2\n",
       "Bahamas                                  2\n",
       "Somalia                                  2\n",
       "Namibia                                  2\n",
       "Paraguay                                 2\n",
       "Rwanda                                   2\n",
       "Zimbabwe                                 2\n",
       "Chile                                    2\n",
       "Macedonia, Republic of                   2\n",
       "Bulgaria                                 2\n",
       "Philippines                              2\n",
       "Angola                                   1\n",
       "Myanmar                                  1\n",
       "Niger                                    1\n",
       "Iraq                                     1\n",
       "Greece                                   1\n",
       "Côte d'Ivoire                            1\n",
       "Benin                                    1\n",
       "Argentina                                1\n",
       "Lebanon                                  1\n",
       "Qatar                                    1\n",
       "Sri Lanka                                1\n",
       "Hong Kong                                1\n",
       "Indonesia                                1\n",
       "Anguilla                                 1\n",
       "Cameroon                                 1\n",
       "Romania                                  1\n",
       "Mozambique                               1\n",
       "Nepal                                    1\n",
       "Malaysia                                 1\n",
       "Name: location, Length: 91, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countryCounts=replies[\"location\"].value_counts()\n",
    "countries=[]\n",
    "for x in range(len(countryCounts)):    \n",
    "    countryCode=replies[\"location\"].value_counts().index[x]\n",
    "    if(countryCode!=\"unknown\"):\n",
    "        countries.append(pycountry.countries.get(alpha_2=countryCode).name)\n",
    "    else:\n",
    "        countries.append(\"unknown\")\n",
    "        \n",
    "countryCounts.index=countries\n",
    "countryCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a4f59603c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(countryCounts)-2),countryCounts.values[2:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chart above, we ignore the bars for the United States and unknown locations as it's already apparent they take a third of locations each (and the bar chart would not be very useful since their values are much larger than the rest). We can see that for the reamining countries, the distribution is not very even, most of the tweets are from United Kingdom, Canada, Bangladesh, Australia and India. There are some countries like Malaysia which only have 1 tweet representing them. Also note that out of the 195 countries in the world, only 91 are being represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3b) Preprocessing the tweets for sentiment analaysis</h2><br>\n",
    "\n",
    "We want to compute the sentiment values for the reply tweets. However, a lot of these tweets have characters/symbols that don't contribute to these calculation and we want to ensure that they do not hinder the results. Therefore we will replace/remove such symbols before doing any calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removes the @ symbol commonly used for replies in tweets\n",
    "def RemoveAtSymbol(s):    \n",
    "    split=s.split(\" \")\n",
    "    newS=[]\n",
    "    for x in split:\n",
    "        if(not x.startswith(\"@\")):\n",
    "            newS.append(x+\" \")\n",
    "    return \"\".join(newS)[:-1]\n",
    "\n",
    "#If a tweet contains several @ symbols, removes all of them (including the name of the user they were addressed to)\n",
    "def RemoveAllAtSymbol(s):\n",
    "    while(s.startswith(\"@\")):\n",
    "        s=RemoveAtSymbol(s)\n",
    "    return s\n",
    "\n",
    "def RemoveNewLine(s):\n",
    "    return s.replace(\"\\n\",\"\")\n",
    "\n",
    "def RemoveHashtag(s):\n",
    "    return s.replace(\"#\",\"\")\n",
    "\n",
    "def RemovePunctuation(s):\n",
    "    return s.replace(\".\",\"\").replace(\",\",\"\")\n",
    "\n",
    "#Twitter encodes the symbols &,<,> as codes. \n",
    "def RemoveEncodedChars(s):\n",
    "    return s.replace(\"&amp;\",\"and\").replace(\"&lt;\",\"<\").replace(\"&gt;\",\">\")\n",
    "\n",
    "#Returns true if the string is a http link\n",
    "def IsLink(s):\n",
    "    return s.startswith(\"https://\")\n",
    "\n",
    "def RemoveLink(s):\n",
    "    split=s.split(\" \")\n",
    "    newS=[]\n",
    "    for x in split:\n",
    "        if(not x.startswith(\"https://\")):\n",
    "            newS.append(x+\" \")\n",
    "    return \"\".join(newS)[:-1]\n",
    "\n",
    "#Emoji characters are not needed, they are not contained in the standard ASCII values\n",
    "def RemoveEmojis(s):\n",
    "    li=[]\n",
    "    for x in s:\n",
    "        if(ord(x)<=256):\n",
    "            li.append(x)\n",
    "    return \"\".join(li).strip()\n",
    "  \n",
    "#Performs all of the operations above and creates a list of reply texts\n",
    "def ProcessReplies(replies):\n",
    "    li=[]\n",
    "    for i in range(len(replies)):\n",
    "        s=RemoveEmojis(RemoveLink(RemoveEncodedChars(RemoveAllAtSymbol(RemovePunctuation(RemoveNewLine(replyTexts.iloc[i])))))).strip()\n",
    "\n",
    "        if(IsLink(s)):\n",
    "            s=\"\"\n",
    "            \n",
    "        li.append(s)\n",
    "    return li           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replyTexts=replies[\"full_text\"]\n",
    "replyTexts=ProcessReplies(replyTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>3c) Computing sentiment values</h2><br>\n",
    "\n",
    "We compute the sentiment values using the textblob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentValues=[]\n",
    "\n",
    "#For every reply, calculates the sentiment value and adds it to a list\n",
    "for i in range(len(replyTexts)):    \n",
    "    blob = TextBlob(replyTexts[i])\n",
    "    avgSentiment=0\n",
    "    if(len(blob.sentences)>0): #len==0 means no sentiment can be calculated\n",
    "        avgSentiment=0\n",
    "        for sentence in blob.sentences:\n",
    "            avgSentiment+=sentence.sentiment.polarity\n",
    "        avgSentiment/=len(blob.sentences)\n",
    "    sentimentValues.append(avgSentiment)\n",
    "    \n",
    "replies[\"sentiment\"]=sentimentValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the mean of all sentiments by country\n",
    "sentiment_data=replies[[\"sentiment\",\"location\"]].groupby(\"location\").mean()\n",
    "\n",
    "#Get rid of tweets with unkown locations\n",
    "sentiment_data.drop(\"unknown\",inplace=True)\n",
    "\n",
    "locations=sentiment_data.index\n",
    "sentiments=sentiment_data[\"sentiment\"]\n",
    "\n",
    "sentiment_values=pd.Series(data=sentiments,index=locations)\n",
    "\n",
    "countries=[]\n",
    "#For all sentiment values, convert their location to Alpha 3 code\n",
    "for x in range(len(sentiment_values)):    \n",
    "    index=sentiment_values.index[x]\n",
    "    if(index==\"UK\"):\n",
    "        countries.append(\"GBR\")\n",
    "    else:\n",
    "        countries.append(pycountry.countries.get(alpha_2=index).alpha_3)\n",
    "        \n",
    "sentiment_values=pd.Series(data=sentiment_values.values,index=countries)\n",
    "\n",
    "#Add countries that were not represented in the tweets and give them an average sentiment of 0\n",
    "for k in pycountry.countries.indices[\"alpha_3\"].keys():\n",
    "    if(not k in sentiment_values.keys()):\n",
    "        sentiment_values=sentiment_values.append(pd.Series(data=0,index=[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Visualising the data</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_coord = [51.050407, 13.737262]\n",
    "map_path = 'data/countries.geo.json'\n",
    "json_data = json.load(open(map_path))\n",
    "world_map = folium.Map(location=center_coord, tiles='Mapbox Bright', zoom_start=1)\n",
    "\n",
    "world_map.choropleth(\n",
    "    geo_data = map_path,\n",
    "    data = sentiment_values,    \n",
    "    fill_color = 'RdBu', \n",
    "    fill_opacity = 0.5, \n",
    "    line_opacity = 0.2,\n",
    "    line_color = 'black',\n",
    "    key_on = 'feature.id',\n",
    "    legend_name = \"Sentiment of Trump tweets\",\n",
    "    highlight = True\n",
    ")   \n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Roadmap to Milestone 3</h1>\n",
    "\n",
    "This section contains all the tasks that we intend to complete for the 3rd milestone.\n",
    "\n",
    "\n",
    "<h3>Plot a world map for each topic</h3>\n",
    "<h3>Plan for presentation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification of Trump's tweets by topic</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering of tweets will be implemented via topic modeling, more specifically via Latent Dirichlet Allocation (in short LDA). In LDA the documents of a corpus are generated according to a mixture of (dirichlet) word distributions over a vocabulary of fixed size. Guiding intuition, such word distributions are named \"topics\". Each document is characterized by a per-document (dirichlet) topic distribution and a total number of words N. Each word of the document is generated by first sampling a topic assignment out of the per-document distribution and then by sampling a word out of the word distribution. The machine learning task is to backtrack this process to \"infer\" the latent topic distribution of the document. Given the model and the per-document distribution, a corpus can be fully characterized (up to words order, since documents are bag-of-words). This entails that if the number of topics K is s.t. K << D, where D is the length of the dictionary, then under LDA every document has a more succint representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arsal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1ae531e34275>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing has been performed iteratively: at each step, the output of simple functions and tokenization was analyzed seeking for the most occurring incorrect tokens. After determining the cause of the incorrect tokenization, a rule was written to fix it. This process was repeated until, after a thorough inspection, most of the tokens were meaningful. Particular care was devoted to this phase since the performance of clustering algorithms generally depends on the underlying data. The pre-processing includes: shifting to lower case, stripping links, dates and time. Punctuation was ignored except for \".\" and \"'\" since many proper names include these symbols (single or multiple periods are still ignored however). Other accepted symbols are \"@\" and \"#\" at the begin of a word. Stopwords are stripped out. Finally, stemming was applied, even though we found out some cases of ambiguity and reduced readability. We decided to deploy stemming anyway since the various verb conjugations such as \"said\" and \"say\" were a far bigger issue than some small inconvenience. Moreover, readability was maintained by keeping track of both processed and unprocessed tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process a single tweet\n",
    "def preProcess_procedure(tweet):\n",
    "    original = tweet\n",
    "    tweet = re.sub(r'([a-z]+)\\.([A-Z].+)', '\\1 \\2', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'(\\d+[:\\/,\\.])+\\d+ *([ap]\\.?m\\.?)?', '', tweet)\n",
    "    tokenizer = RegexpTokenizer(r'[a-z_@#][a-z0-9_\\'\\.]+[a-z0-9_]')\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    stopwords = get_stop_words('en')\n",
    "    stopwords.append('amp')\n",
    "    p_stemmer = PorterStemmer()\n",
    "    tweet = [i for i in tweet if not i in stopwords]\n",
    "    tweet = [p_stemmer.stem(i) for i in tweet]\n",
    "    #print(original, \"=>\", tweet)\n",
    "    return tweet\n",
    "    \n",
    "# Pre-process a list of tweets\n",
    "def preProcess(tweets, ratio=0.8):\n",
    "    tweets.index = range(len(tweets))\n",
    "    indices = np.random.permutation(tweets.index)\n",
    "    train_len = int(len(tweets) * ratio)\n",
    "    train_indices = indices[:train_len]\n",
    "    test_indices = indices[train_len:]\n",
    "    orig_test = tweets[test_indices]\n",
    "    tweets = tweets.map(preProcess_procedure)\n",
    "    dictionary = corpora.Dictionary(tweets)\n",
    "    train = [dictionary.doc2bow(tweets[tweet]) for tweet in train_indices]\n",
    "    test = [dictionary.doc2bow(tweets[tweet]) for tweet in test_indices]\n",
    "    return train, test, orig_test, dictionary\n",
    "    \n",
    "train, ltest, orig_test, dictionary = preProcess(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right after feature pre-processing, a crucial step in clustering is determining the number of clusters. We started with a non-small number of topics (k = 100) attempting to first overfit the model to the training set (but still keeping a reasonable amount of topics for manual inspection) and then to progressively reduce the number of topics to increase performance. The goodness of fit was measured by first splitting in training and testing samples, inferring a test tweet's most probable topic and then assigning the tweet to that topic (hard clustering). However, this method seemed to be slow since every model seemed to derive small topics of approximately the same size (10 ~ 80 tweets each, where the biggest topic was 4% of the whole dataset). While selecting the number of clusters just so that the dataset would be nicely partitioned in bigger chunks was tempting, we were not sure whether a small cluster could still have a great impact in clustering. To this aim, many of the 100 topics were inspected and we found out that some topics were fairly similar (for example all topics addressing Ted Cruz were divided in several topics). \n",
    "\n",
    "At this point, our aim became minimizing the \"uncertainty\" of the per-tweet topic distributions (i.e. by having higher maxima). In order to do so, recall that in LDA a test sample document of D features is reduced to a K-vector in the K-topic space. We had the idea of computing the silhouette function in this space to improve goodness of fit. We decided to implement that using centroids (the average point of each cluster) and euclidian distance. Another test that would be interesting to make is to use the total variation distance since every point in the topic space is a probability distribution over K topics. A completely different approach that was implemented was to set the topic assignment zi as the center of the cluster. The total variation distance between a tweet and all zi is then simply 1 - di, where di is the i-th component of the tweet K-vector. This approach was not thoroghly tested but it seemed to produce similar results. As you may note below, the maximum silhouette value was achieved for k = 2. This behavior is certainly suspicious. Nonetheless, the resulting clusters seems to be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing silhouette in a K-dimensional topic space, where each document is reduced to a K-vector. Note that documents are\n",
    "# hard assigned to the topic with the highest probability. The centroids are computed as the average point of the cluster.\n",
    "\n",
    "\"\"\"\n",
    "def silhouette(lda, tweet):\n",
    "    kvector = [x[1] for x in sorted(lda[tweet], key=lambda tup: tup[1], reverse=True)]\n",
    "    if (len(kvector) < 2):\n",
    "        return 1\n",
    "    neigh_dist = 1 - kvector[1]\n",
    "    same_dist = 1 - kvector[0]\n",
    "    return (neigh_dist - same_dist) / neigh_dist\n",
    "\"\"\"\n",
    "\n",
    "def silhouette(other, same):\n",
    "    return (other - same) / max(other, same)\n",
    "\n",
    "# Given a model, infer and assign a topic to every tweet\n",
    "def buildHardClusters(lda, ltest):\n",
    "    docs = [lda[tweet] for tweet in ltest if len(tweet) > 0]\n",
    "    clusters = [max(doc, key=lambda tup: tup[1]) for doc in docs]\n",
    "    dfTest = pd.DataFrame(columns=['Topic', 'Document'])\n",
    "    dfTest['Topic'] = [c[0] for c in clusters]\n",
    "    dfTest['Document'] = docs\n",
    "    return dfTest\n",
    "\n",
    "# From a cluster of tweets, compute the average point\n",
    "def buildCentroid(cluster):\n",
    "    dic = defaultdict(float)\n",
    "    for doc in cluster['Document']:\n",
    "        for comp in doc:\n",
    "            dic[comp[0]] += comp[1]\n",
    "    return [(t, p / len(cluster)) for t, p in dic.items()]\n",
    "\n",
    "# Difference of two vectors. We had to implement this since\n",
    "# zero entries are not stored in the vectors to save space\n",
    "def diff(doc1, doc2):\n",
    "    d = defaultdict(float)\n",
    "    for comp in doc1:\n",
    "        d[comp[0]] = comp[1]\n",
    "    for comp in doc2:\n",
    "        d[comp[0]] -= comp[1]\n",
    "    tot = 0\n",
    "    for k, v in d.items():\n",
    "        tot += v**2\n",
    "    return np.sqrt(tot)\n",
    "    \n",
    "def compute_silhouette(tweet, centroids):\n",
    "    same_dist = diff(tweet['Document'], centroids[tweet['Topic']]) \n",
    "    # Find nearest neighbor\n",
    "    neigh_dist = 999\n",
    "    for i in range(len(centroids)):\n",
    "        if i == tweet['Topic']:\n",
    "            continue\n",
    "        dist = diff(tweet['Document'], centroids[i])\n",
    "        if dist < neigh_dist:\n",
    "            neigh_dist = dist\n",
    "    return silhouette(neigh_dist, same_dist)\n",
    "\n",
    "# Grid Search method to determine k by maximizing the silhouette function.\n",
    "def grid_search_k(train, ltest, dictionary, start_k=2, lim=100):\n",
    "    res = pd.Series(index=range(start_k, lim + 1))\n",
    "    for k in range(start_k, lim + 1):\n",
    "        alda = models.ldamodel.LdaModel(train, num_topics=k, id2word = dictionary, passes=20)\n",
    "        dfTest = buildHardClusters(alda, ltest)\n",
    "        centroids = dfTest.groupby('Topic').apply(buildCentroid)\n",
    "        # Compute Average Silhouette\n",
    "        tot = 0\n",
    "        part_sil = dfTest.apply(compute_silhouette, args=(centroids,), axis=1)\n",
    "        sil = sum(part_sil) / len(dfTest)\n",
    "        print(\"k =\", k, \",\", \"silhouette =\", sil)\n",
    "        res[k] = sil\n",
    "    return res\n",
    "\n",
    "gsk = grid_search_k(train, ltest, dictionary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Output of previous cell\n",
    "k = 2 , silhouette = 0.7600314381266555\n",
    "k = 3 , silhouette = 0.6752275340791493\n",
    "k = 4 , silhouette = 0.6140042422538549\n",
    "k = 5 , silhouette = 0.5843056284001248\n",
    "k = 6 , silhouette = 0.5634937808802221\n",
    "k = 7 , silhouette = 0.5444461357828932\n",
    "k = 8 , silhouette = 0.5345121291019872\n",
    "k = 9 , silhouette = 0.5124009337536873\n",
    "k = 10 , silhouette = 0.5040697061853416\n",
    "k = 11 , silhouette = 0.49080282616663023\n",
    "k = 12 , silhouette = 0.4790508910985389\n",
    "k = 13 , silhouette = 0.4914963311174731\n",
    "k = 14 , silhouette = 0.458577477784802\n",
    "k = 15 , silhouette = 0.46437586037353934\n",
    "k = 16 , silhouette = 0.4657706101174737\n",
    "k = 17 , silhouette = 0.46563137263218274\n",
    "k = 18 , silhouette = 0.45814675040136654\n",
    "k = 19 , silhouette = 0.44681182325184804\n",
    "k = 20 , silhouette = 0.4404248632737023\n",
    "k = 21 , silhouette = 0.450479056001983\n",
    "k = 22 , silhouette = 0.43428431342093504\n",
    "k = 23 , silhouette = 0.4417843784799455\n",
    "k = 24 , silhouette = 0.43143907239833473\n",
    "k = 25 , silhouette = 0.4516537367356941\n",
    "k = 26 , silhouette = 0.4329791473329979\n",
    "k = 27 , silhouette = 0.4232088589462763\n",
    "k = 28 , silhouette = 0.42385696104066134\n",
    "k = 29 , silhouette = 0.4389556460149199\n",
    "k = 30 , silhouette = 0.41114286781053805\n",
    "k = 31 , silhouette = 0.4178586544421491\n",
    "k = 32 , silhouette = 0.42699089550760255\n",
    "k = 33 , silhouette = 0.42817235267779397\n",
    "k = 34 , silhouette = 0.4019865036449782\n",
    "k = 35 , silhouette = 0.4219532322367825\n",
    "k = 36 , silhouette = 0.4239512264186842\n",
    "k = 37 , silhouette = 0.41320304717442036\n",
    "k = 38 , silhouette = 0.40899663868518815\n",
    "k = 39 , silhouette = 0.4196022670687089\n",
    "k = 40 , silhouette = 0.4133526097045497\n",
    "k = 41 , silhouette = 0.4041296687698144\n",
    "k = 42 , silhouette = 0.39581897494439566\n",
    "k = 43 , silhouette = 0.41190368809835587\n",
    "k = 44 , silhouette = 0.403219261494485\n",
    "k = 45 , silhouette = 0.4089286365742654\n",
    "k = 46 , silhouette = 0.4101530433588281\n",
    "k = 47 , silhouette = 0.4043158558695186\n",
    "k = 48 , silhouette = 0.3783173863517066\n",
    "k = 49 , silhouette = 0.4163419644987627\n",
    "k = 50 , silhouette = 0.4002452419960091\n",
    "k = 51 , silhouette = 0.40872221149401494\n",
    "k = 52 , silhouette = 0.4078669034314252\n",
    "k = 53 , silhouette = 0.403135794407241\n",
    "k = 54 , silhouette = 0.399162946194608\n",
    "k = 55 , silhouette = 0.39601282052952436\n",
    "k = 56 , silhouette = 0.3851635102946601\n",
    "k = 57 , silhouette = 0.40398259675393\n",
    "k = 58 , silhouette = 0.39689044755223585\n",
    "k = 59 , silhouette = 0.3864906090049179\n",
    "k = 60 , silhouette = 0.40839095194404607\n",
    "k = 61 , silhouette = 0.4057389955374202\n",
    "k = 62 , silhouette = 0.40118669701774756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(train, num_topics=2, id2word = dictionary, passes=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hard clustering: map each tweet to the topic with max probability\n",
    "clusters = [max(lda[tweet], key=lambda tup: tup[1]) for tweet in ltest if len(tweet) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of clusters\n",
    "w = [1 / len(clusters)] * len(clusters)\n",
    "pd.Series([c[0] for c in clusters]).hist(bins=100, weights=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTest = pd.DataFrame(columns=['Max Topic', 'Text'])\n",
    "orig_test.index = range(len(orig_test))\n",
    "dfTest['Max Topic'] = [c[0] for c in clusters]\n",
    "dfTest['Text'] = [orig_test[i] for i in range(len(orig_test)) if len(ltest[i]) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two derived topics seems to have an interesting side-effect: the classifier acts as Trump's rant detector. It infers 1 when Trump attacks a controversial issue in his tweet, generally defending himself or belittling his opponents (Clinton, Obama, the Democrats, the FBI CEO...) in a manner that might resemble an angry rant, targeted to awaken people's anger. It is not unusual to read keywords such as \"FAKE NEWS\" or \"rigged system\". \n",
    "\n",
    "Examples: \n",
    "1. \"FAKE NEWS media knowingly doesn't tell the truth. A great danger to our country. The failing @nytimes has become a joke. Likewise @CNN. Sad!\"\n",
    "\n",
    "2. \"What is our country coming to when a judge can halt a Homeland Security travel ban and anyone, even with bad intentions, can come into U.S.?\"\n",
    "\n",
    "3. \"James Comey will be replaced by someone who will do a far better job, bringing back the spirit and prestige of the FBI.\"\n",
    "\n",
    "4. \"Karen Handle's opponent in #GA06 can't even vote in the district he wants to represent....\"\n",
    "\n",
    "5. \"Everybody is asking why the Justice Department (and FBI) isn't looking into all of the dishonesty going on with Crooked Hillary & the Dems.. \"\n",
    "\n",
    "In contrast, we observe a completely different pattern in the other cluster. Trump glorifies and thanks either his supporters, his government or even his family. He celebrates having met \"great people\" or festivities. In this cluster we observe a Trump that inspires people hope. Frequent keywords are \"honor\", \"hope\", \"pray\", \"God\", \"kind\", \"jobs\"...\n",
    "\n",
    "Examples: \n",
    "\n",
    "1. \"My warmest condolences and sympathies to the victims and families of the terrible Las Vegas shooting. God bless you\"\n",
    "\n",
    "2. \"Getting ready to celebrate the 4th of July with a big crowd at the White House. Happy 4th to everyone. Our country will grow and prosper! \"\n",
    "\n",
    "3. \"Today on #NationalAgDay, we honor our great American farmers & ranchers. Their hard work & dedication are ingrained…\"\n",
    "\n",
    "4. \"Thank you for such a wonderful and unforgettable visit, Prime Minister @Netanyahu and @PresidentRuvi.\"\n",
    "\n",
    "5. \"Nick Adams, \"Retaking America\"  \"Best things of this presidency aren't reported about. Convinced this will be perhaps best presidency ever.\" \"\n",
    "\n",
    "For more examples please look through files \"topic0.txt\" and \"topic1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testTopic(dfTest, i):\n",
    "    group = dfTest[dfTest['Max Topic'] == i]['Text']\n",
    "    group.apply(lambda x: print(x, \"\\n\"))\n",
    "    print(\"frequency:\", len(group) / len(dfTest) * 100, \"%\")\n",
    "\n",
    "# To check out the output of this cell consult \"Topic0.txt\"\n",
    "testTopic(dfTest, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To check out the output of this cell consult \"Topic1.txt\"\n",
    "testTopic(dfTest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.show_topics(num_words=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Output of previous cell\n",
    "[(0,\n",
    "  '0.021*\"great\" + 0.016*\"thank\" + 0.014*\"today\" + 0.011*\"presid\" + 0.009*\"will\" + 0.008*\"meet\" + 0.008*\"honor\" + 0.007*\"nation\" + 0.007*\"state\" + 0.007*\"america\" + 0.006*\"day\" + 0.006*\"unit\" + 0.006*\"@realdonaldtrump\" + 0.005*\"trump\" + 0.005*\"peopl\" + 0.004*\"welcom\" + 0.004*\"american\" + 0.004*\"@whitehous\" + 0.004*\"happi\" + 0.004*\"order\" + 0.004*\"just\" + 0.004*\"first\" + 0.004*\"u.\" + 0.004*\"forward\" + 0.004*\"militari\" + 0.004*\"join\" + 0.004*\"new\" + 0.003*\"women\" + 0.003*\"look\" + 0.003*\"@potu\" + 0.003*\"live\" + 0.003*\"@flotu\" + 0.003*\"minist\" + 0.003*\"congratul\" + 0.003*\"wonder\" + 0.003*\"countri\" + 0.003*\"full\" + 0.003*\"prime\" + 0.003*\"famili\" + 0.003*\"#usa\" + 0.003*\"job\" + 0.003*\"texa\" + 0.003*\"morn\" + 0.003*\"secur\" + 0.003*\"white\" + 0.003*\"work\" + 0.003*\"help\" + 0.003*\"world\" + 0.002*\"melania\" + 0.002*\"men\" + 0.002*\"law\" + 0.002*\"watch\" + 0.002*\"protect\" + 0.002*\"attack\" + 0.002*\"court\" + 0.002*\"korea\" + 0.002*\"hous\" + 0.002*\"leader\" + 0.002*\"safe\" + 0.002*\"everyon\" + 0.002*\"announc\" + 0.002*\"secretari\" + 0.002*\"god\" + 0.002*\"terror\" + 0.002*\"beauti\" + 0.002*\"hero\" + 0.002*\"respond\" + 0.002*\"presidenti\" + 0.002*\"must\" + 0.002*\"host\" + 0.002*\"head\" + 0.002*\"@vp\" + 0.002*\"@scavino45\" + 0.002*\"much\" + 0.002*\"remark\" + 0.002*\"get\" + 0.002*\"gener\" + 0.002*\"execut\" + 0.002*\"donald\" + 0.002*\"support\" + 0.002*\"time\" + 0.002*\"bless\" + 0.002*\"readi\" + 0.002*\"leav\" + 0.002*\"mani\" + 0.002*\"togeth\" + 0.002*\"serv\" + 0.002*\"ban\" + 0.002*\"discuss\" + 0.002*\"brave\" + 0.002*\"prayer\" + 0.002*\"see\" + 0.002*\"offic\" + 0.002*\"wall\" + 0.002*\"rememb\" + 0.002*\"proud\" + 0.002*\"hurrican\" + 0.002*\"veteran\" + 0.002*\"right\" + 0.002*\"even\"'),\n",
    " (1,\n",
    "  '0.019*\"will\" + 0.012*\"great\" + 0.010*\"news\" + 0.009*\"fake\" + 0.008*\"big\" + 0.008*\"job\" + 0.007*\"tax\" + 0.007*\"now\" + 0.007*\"peopl\" + 0.006*\"get\" + 0.006*\"republican\" + 0.006*\"year\" + 0.006*\"media\" + 0.006*\"just\" + 0.006*\"democrat\" + 0.006*\"u.\" + 0.005*\"make\" + 0.005*\"countri\" + 0.005*\"senat\" + 0.005*\"elect\" + 0.005*\"vote\" + 0.005*\"time\" + 0.005*\"american\" + 0.005*\"work\" + 0.005*\"mani\" + 0.005*\"russia\" + 0.004*\"trump\" + 0.004*\"healthcar\" + 0.004*\"cut\" + 0.004*\"want\" + 0.004*\"obamacar\" + 0.004*\"dem\" + 0.004*\"@foxandfriend\" + 0.004*\"never\" + 0.004*\"one\" + 0.004*\"even\" + 0.004*\"deal\" + 0.004*\"can\" + 0.004*\"bad\" + 0.003*\"stori\" + 0.003*\"report\" + 0.003*\"back\" + 0.003*\"hard\" + 0.003*\"much\" + 0.003*\"america\" + 0.003*\"come\" + 0.003*\"good\" + 0.003*\"total\" + 0.003*\"win\" + 0.003*\"look\" + 0.003*\"made\" + 0.003*\"bill\" + 0.003*\"new\" + 0.003*\"like\" + 0.003*\"take\" + 0.003*\"obama\" + 0.003*\"fail\" + 0.003*\"sinc\" + 0.003*\"talk\" + 0.003*\"go\" + 0.003*\"must\" + 0.003*\"said\" + 0.003*\"hillari\" + 0.003*\"thing\" + 0.003*\"ever\" + 0.003*\"state\" + 0.003*\"hous\" + 0.003*\"china\" + 0.003*\"border\" + 0.002*\"north\" + 0.002*\"clinton\" + 0.002*\"way\" + 0.002*\"say\" + 0.002*\"@foxnew\" + 0.002*\"trade\" + 0.002*\"record\" + 0.002*\"high\" + 0.002*\"administr\" + 0.002*\"market\" + 0.002*\"korea\" + 0.002*\"realli\" + 0.002*\"better\" + 0.002*\"help\" + 0.002*\"stock\" + 0.002*\"happen\" + 0.002*\"last\" + 0.002*\"secur\" + 0.002*\"plan\" + 0.002*\"russian\" + 0.002*\"done\" + 0.002*\"interview\" + 0.002*\"nation\" + 0.002*\"rate\" + 0.002*\"repeal\" + 0.002*\"let\" + 0.002*\"replac\" + 0.002*\"see\" + 0.002*\"need\" + 0.002*\"chang\" + 0.002*\"massiv\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show Clustering in detail\n",
    "i = 0\n",
    "for tweet in ltest:\n",
    "    print(orig_test.iloc[i], \"\\n\")\n",
    "    print(sorted(lda[tweet], key=lambda tup: tup[1], reverse=True), \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda.save(fname=\"LDAModel2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfTest.to_csv(\"Test Sample_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
